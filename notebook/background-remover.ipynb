{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "To remove objects from images, there are several algorithms:\n",
    "\n",
    "* **Clustering**\n",
    "    * It usually partition the image into several clusters.\n",
    "    * K-means is a well known method.\n",
    "* **Thresholding**\n",
    "    * The simplest method.\n",
    "    * The key is to select a threshold value and then compare to each pixel.\n",
    "* **Region Growing**\n",
    "    * Mainly relies on the that the neighbors in same region should be similar.\n",
    "* **Deep Learing**\n",
    "    * It has an enormous achievement on this field.\n",
    "    * Usually be implemented with convolutional layers.\n",
    "\n",
    "All of them are very powerful and interesting, but we'll implement the remover with **deep learing**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Network)\n",
    "\n",
    "In deep learning, tasks about image are often solved with **CNN**. <br>\n",
    "CNN has some powerful benefits:\n",
    "* It takes important features from images, such as edges.\n",
    "* In deep learning, it reduces the number of parameters, but has better performance.\n",
    "* Network can be calculated on GPUs more faseter than on CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explain\n",
    "\n",
    "There are many models for image segmentation made by well known organizations and researchers. <br>\n",
    "We'll use **U-Net** in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net\n",
    "\n",
    "In traditional models, layers are usually connected to the next one. <br>\n",
    "While more maxpooling layers inputs go through, the more features are lost.\n",
    "\n",
    "U-Net solve this problem in a clever way. <br>\n",
    "<img src=\"https://img-blog.csdn.net/20181022150306666?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dpdGh1Yl8zNjkyMzQxOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"70%\">\n",
    "\n",
    "It add outputs from encoder to layers of decoder directly, so the decoder can use more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building model, we should prepare our data first. <br>\n",
    "We'll use images from **COCO dataset** to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download train, validation and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget http://images.cocodataset.org/zips/test2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip train2017.zip\n",
    "!unzip val2017.zip\n",
    "!unzip test2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use COCO dataset for training, we need **annotation** files to get masks of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have three datasets and several json files in annotation folder. <br>\n",
    "Next, we have to preprocess the images by creating mask images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data information from annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "# annotations/instances_{dataset}2017.json\n",
    "path = join(home, \"annotations/instances_train2017.json\")\n",
    "data = COCO(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need images that contain person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import join\n",
    "\n",
    "i = 0\n",
    "images, masks = [], []\n",
    "for img in data.loadImgs(data.getImgIds()):\n",
    "    \n",
    "    valid = False\n",
    "    anns = data.loadAnns(data.getAnnIds(imgId))\n",
    "\n",
    "    mask = np.zeros((img[\"height\"], img[\"width\"]), dtype=np.byte)\n",
    "    for ann in anns:\n",
    "        \n",
    "        # category id of person is 1\n",
    "        if ann[\"category_id\"] == 1:\n",
    "            seg = data.annToMask(ann)\n",
    "            mask += seg\n",
    "            valid = True\n",
    "    \n",
    "    # if contains person\n",
    "    if valid:\n",
    "        \n",
    "        file_name = img[\"file_name\"]\n",
    "        # {dataset}2017\n",
    "        frame = cv2.imread(\"train2017\" + file_name)\n",
    "        # frames/{dataset}\n",
    "        cv2.imwrite(join(\"frames/train\", file_name), image)\n",
    "        # masks/{dataset}\n",
    "        cv2.imwrite(join(\"masks/train\", file_name), mask)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use 16000 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image shape\n",
    "img_shape = (256, 256)\n",
    "batch_size = 32\n",
    "num_train = 500 * batch_size\n",
    "num_val = 50 * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from os.path import join, expanduser\n",
    "import numpy as np\n",
    "\n",
    "# define our own pipeline\n",
    "def generator(frames_path, masks_path, batch_size, img_shape, num_data=None):\n",
    "    frames_path = glob.glob(join(frames_path, \"*.jpg\"))\n",
    "    frames_path = sorted(frames_path, key=lambda path: int(path.split('/')[-1].split('.')[0]))\n",
    "    masks_path = glob.glob(join(masks_path, \"*.jpg\"))\n",
    "    masks_path = sorted(masks_path, key=lambda path: int(path.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    # use all samples\n",
    "    if num_data is None:\n",
    "        num_data = len(frames_path)\n",
    "    order = np.arange(num_data)\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    base = 0\n",
    "    while True:\n",
    "\n",
    "        if base == num_data - 1:\n",
    "            np.random.shuffle(order)\n",
    "            base = 0\n",
    "\n",
    "        frames, masks = [], []\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # this ensure that we get correct number of samples\n",
    "            idx = order[(base + i) % num_data]\n",
    "            \n",
    "            frame = cv2.imread(frames_path[idx]).astype(np.float32)\n",
    "            frame = cv2.resize(frame, img_shape)\n",
    "            mask = cv2.imread(masks_path[idx], cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "            mask = cv2.resize(mask, img_shape).reshape(img_shape + (1, ))\n",
    "\n",
    "            frames.append(frame)\n",
    "            masks.append(mask)\n",
    "\n",
    "        base += batch_size\n",
    "\n",
    "        # yield makes function iterateble\n",
    "        yield np.array(frames), np.array(masks)\n",
    "        del frames, masks, frame, mask, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frames_path = \"frames/train\"\n",
    "train_masks_path = \"masks/train\"\n",
    "train_generator = generator(train_frames_path, train_masks_path, batch_size, img_shape, num_train)\n",
    "\n",
    "val_frames_path = \"frames/val\"\n",
    "val_masks_path = \"masks/val\"\n",
    "val_generator = generator(val_frames_path, val_masks_path, batch_size, img_shape, num_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segmentation-models is an library that provides several models for image segmentation. <br>\n",
    "You can see more details on [github](https://github.com/qubvel/segmentation_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "model = sm.Unet(\"resnet34\", input_shape=img_shape + (3, ))\n",
    "model.compile(\"Adam\", loss=sm.losses.bce_jaccard_loss, metrics=[sm.metrics.iou_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 100 eopchs and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=num_train / batch_size, \n",
    "                    epochs=100, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=num_val / batch_size)\n",
    "\n",
    "model.save_weights(\"unet-100.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(val_generator, \n",
    "                         steps=num_val / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def frame_generator(frames_path, img_shape):\n",
    "    \n",
    "    frames_path = glob.glob(join(frames_path, \"*.jpg\"))\n",
    "    np.random.shuffle(frames_path)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        \n",
    "        if i == len(frames_path):\n",
    "            i = 0\n",
    "        \n",
    "        frame = cv2.imread(frames_path[i])\n",
    "        frame = cv2.resize(frame, img_shape)\n",
    "        yield frame\n",
    "        i += 1\n",
    "\n",
    "test_generator = frame_generator(\"frames/test\", img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frame = next(test_generator)\n",
    "res = model.predict(np.array([frame]))[0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "# cv2 read image in BGR mode\n",
    "plt.imshow(frame[:,:,::-1])\n",
    "\n",
    "# convert mask into three-chennel image\n",
    "res = np.repeat(res, 3, axis=2)\n",
    "# set average value as treshold to cut off the foreground\n",
    "np.putmask(frame, res < np.average(res), frame * 0)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(frame[:,:,::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
